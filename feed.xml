<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ginnyintifa.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ginnyintifa.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-21T00:56:34+00:00</updated><id>https://ginnyintifa.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Rethinking Normalization in RNA-seq and ChIP-seq</title><link href="https://ginnyintifa.github.io/blog/2025/distill_4/" rel="alternate" type="text/html" title="Rethinking Normalization in RNA-seq and ChIP-seq"/><published>2025-02-15T00:00:00+00:00</published><updated>2025-02-15T00:00:00+00:00</updated><id>https://ginnyintifa.github.io/blog/2025/distill_4</id><content type="html" xml:base="https://ginnyintifa.github.io/blog/2025/distill_4/"><![CDATA[<h2 id="foreword">Foreword</h2> <p>I recently work on a project where the data is primarily from an enrichment-based assay. And this is the first time I work with such type of NGS data. Naturally, normalization is the first step after I get a count matrix, and naturally, I would try a library-size-based normalization, i.e. dividing the count of each feature by the total library size (and there could be more sophisticated operations like multiplying by a scalar to make the number looks larger, or accounting for gene length etc, but let’s just call this a library-size-based normalization). But as I think deeper on this general approach, I developed more doubts on it. Is it fair to do so? What if there is some global change between samples? For example in a ChIP-seq experiment, probably in an unlikely scenario, the transcription factor we are looking at binds 2 times more in one sample compared to another sample, after immunoprecipatation, we would get a lot more fragments in that super active sample, how do we justify the idea of libray-size based normaliztion, and how do we check for such a “global” activation? Remember, in reality, as a computational scientist, we only see the numbers in the count matrix, or bam reads, and we don’t have (or shouldn’t have (think double-blined clinical trials)) the knowledge about the ground truth.</p> <h2 id="typical-rna-seq-experiments">Typical RNA-seq experiments</h2> <p>It is always easier to learn from something simpler, and generalize the learning to a more complicated experiment design. In a typcial RNA-seq experiments, we are comparing gene expression beteen sample groups, for example, tumor vs normal, treatment vs control. The differences we <strong>observe</strong> in library sizes could be resulted from differences in the following<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</p> <ul> <li>1 Loadings of RNA</li> <li>2 Nature of RNA sampels (e.g. you got degraded RNA in a sample)</li> <li>3 Library preparation efficiency (adapther ligation, PCR amplification, etc)</li> <li>4 Sequencing variablity (sequencing efficiency)</li> <li>5 Alignment (low quality reads are filtered)</li> <li>6 Human handling errors that are completely random and untrackable.</li> </ul> <p>Let’s tackle them one by one:</p> <ul> <li>1 Loadings of RNA <ul> <li>Typically in the context of differential expression analysis, people would start with the same loadings for all the samples, but even without<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, we can assume proportionality<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> and size up or down the samples to the same library size, so basically we do a library size normalization and compare the per-unit (per million) size counts. Most importantly, this won’t be of biological relevance (i.e. we hope different samples to have the same loadings).</li> </ul> </li> <li>2 Nature of RNA samples (e.g. comparison groups) <ul> <li>Given same loadings (before or after adjustment), it shouldn’t affect the grand total library size. As for compositional biases, we use methods like <a href="https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-3-r25">TMM</a> to take care.</li> </ul> </li> <li>3 Library preparation efficiency (adapther ligation, PCR amplification, etc) <ul> <li>These biases are considered random. We know their existence, but we can’t tell exactly how much they affect without having spike-in samples.</li> </ul> </li> <li>4 Sequencing variablity <ul> <li>Same. These biases are considered random. We know their existence, but we can’t tell exactly how much they affect without having spike-in samples.</li> </ul> </li> <li>5 Alignment (low quality reads are filtered) <ul> <li>Some samples may have more low quality reads than others, and this goes back to our pointer 2.</li> </ul> </li> <li>6 Human handling errors that are completely random and untrackable. <ul> <li>Same as before, this is not of interest and except for library size normalization, we don’t do anything to address it.</li> </ul> </li> </ul> <p>Basically all these biases could potentially change the library size, and usually the experiment is affected by multiple factors. Library-size-based normalization makes us comparing groups in a per unit library size (usually per million) basis, addressing all these biases. Anything that is not random here (except for comparison groups), can be accounted for by batch correction.</p> <p><strong>Long story short, why is it okay to compare in unit library size? Because the factors that result in library size differences are not of our interest.</strong></p> <h2 id="enrichment-based-assays">Enrichment-based assays</h2> <p>Let’s take a look at a general ChIP-seq experiment workflow:</p> <p><img src="/assets/img/CHIP.jpg" alt="ChIP Image"/></p> <p>In the immunoprecipitation (IP) step, DNA that were bound by the target protein (e.g. transcription factor) are grabbed (others are washed out) and subjected to sequencing. Imaging we are comparing group A and B, and we start off with the same amount of cells, we cross link, shear DNA strands, we precipitate and unlink DNA from protein, we reach the sequencing step, we would most likely end up with different amounts of DNA material, when libraries are constructed, similar amount of DNA would be loaded to the sequencer such that we lose the informatoin that how much binding happend in total. <strong>We can not say enrichment of binding in this site is more in A than B, but we can say enrichment of binding in this site is more in A comparing to A’s other binding sites than in B comparing to B’s other binding sites.</strong></p> <p>Because of the addtional IP step in ChIP-seq, there are some other sources biases.</p> <ul> <li>7 Crosslinking efficiency</li> <li>8 Shearing efficiency</li> <li>9 IP efficiency</li> <li>10 DNA recovery efficiency after IP</li> <li>and more</li> </ul> <p>Two major differences from RNA-seq data:</p> <ul> <li>We often supplement our ChIP samples with an “input sample” as background control. <ul> <li>Input samples are processedd in parallel with the ChIP samples but does not undergo immunoprecipitation. Instead, the crosslinked chromatin is reverse-crosslinked, and the DNA is purified directly and sequenced. So it is supposed to be the same as its pairing ChIP sample in all the aspects but for the signals from DNA that are bound to the target proteins.</li> <li>Imaging a chromatin region with no enrichment of binding with the target protein, but it is more open (accessible) compared to other regions. It would be more easily fragmented in the shearing step, and more likely to be bound by proteins non-specifically, including the antibody used in ChIP, also it would be amplified in the sequencing library too. Such regions in the input sample would have higher coverage because they are more accessible. Therefore, input control sample helps us to identify genuine binding sites.</li> </ul> </li> <li>Peaks are the features in ChIP-seq experiments. <ul> <li>In RNA-seq experiments, genes are often the features. In ChIP-seq, we introduce the concept of peaks, which are the regions of signficant enrichment of read counts. Popular tools like MACS2 model the read counts with a poisson distribution and only significantly different from the background (input sample’s distribution if there is the input sample) are considered a peak.</li> <li>After peak calling, you would get a matrix with the rows being peaks, and columns being the samples. The values are read counts mapped to the peaks. You could imagine these reads would be a subset of the total reads. Usually the fraction of Read in Peaks (RiP) is no higher than 20% in transcription factor binding scenario and no more than 60% in histone modifications (need to find reference for this). The reads that are not mapped to any peaks are considered noise that is reflective of the ChIP efficiency.</li> </ul> </li> </ul> <p>Knowing the two details of the data, it makes more sense to normalize against the total reads in peaks than the full library size to also account for the ChIP efficiency bias. As I have already said, at this stage we already gave up studying the global absolute shift of signals, we are doing this in relative terms.</p> <p>Please take a look at the <a href="https://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf">DiffBind tutorial</a><sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> to see even more details in normalization. They compared full library size with read in peaks, and mentioned background normalization, where larger bins are formed. Coupled with TMM, it can yield more robust results.</p> <p>Then how do we know if there is a global binding shift?</p> <p><strong>Experimentally</strong> The best practice to have “spike-in” samples (Orlando, David A., et al. “Quantitative ChIP-Seq normalization reveals global modulation of the epigenome.” Cell reports 9.3 (2014): 1163-1170.), which are made of known quantity of exogenous (mostly drosophila when study human samples) DNA or chromatin. They are added to all the samples in the study right before all the processing steps. Unlike the input controls we talked about, they are also immunoprecipitated. Because we know their quantities (amount of bindings) are equal across all the samples, after all the procedures, they should, in theory, yield the same amount of DNA fragments in the final library. Any difference are for sure technical biases, such as variations in immunoprecipitation efficiency, sequencing depth, or library preparation. And these biases are the same biases the actual samples are suffering from. So if we normalize our human samples with the spike-ins we correct for technical biases and isolate true biological signals.</p> <p><strong>Computationally</strong> In the scenario where no spike-ins are availabily, which is unfortunately mose cases I have seen. Computational methods come to rescue. I found a tool called ChIPseqSpikeInFree (Jin, Hongjian, et al. “ChIPseqSpikeInFree: a ChIP-seq normalization approach to reveal global changes in histone modifications without spike-in.” Bioinformatics 36.4 (2020): 1270-1272.) that could be used to study the global shift question. It was tested to be successful for histon modifications but no systematic benchmark was done for other types of ChIPseq experiments.</p> <h2 id="ps">P.S.</h2> <p>Normalization is a critical yet often overlooked aspect of data analysis. While it may not seem as exciting as employing cutting-edge AI tools—especially in today’s research climate—it remains a cornerstone of robust dicovery.</p> <p>I initially aimed to cover normalization in single-cell RNA-seq and proteomics data analysis as well, including some hands-on examples. However, this blog post has already grown quite lengthy. I’ll save those topics for a future installment, so stay tuned!</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I’m trying to list all the possible sources in the order of the experiment workflow, and trying to be mutually exclusive. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>If the loadings are off by too much, you would imagine the variance of gene counts between samples would be quite different, and all the uncontrollable factors would have influenced the samples at different magnitudes. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>We assume if we use two times the original loading, the reads and counts would be two times the original resutls. This sounds like a strong assumption, but it is also what we assume when we conduct any NGS experiment. It is wild if we repeat the analysis with different loadings and the conclusion changes qualitatively. But it is a good idea to keep the loadings the same between samples, especially when you want to compare them, as footnote 2 points out. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>I don’t fully agree with their recommendation of using full library size. It seems they are still trying to make an absolute differential binding conclusion knowing the raw data trend is explainable by both biological difference and technical artefacts. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Get the basics sorted out.]]></summary></entry><entry><title type="html">Understanding PCA deeper</title><link href="https://ginnyintifa.github.io/blog/2023/distill_3/" rel="alternate" type="text/html" title="Understanding PCA deeper"/><published>2023-06-25T00:00:00+00:00</published><updated>2023-06-25T00:00:00+00:00</updated><id>https://ginnyintifa.github.io/blog/2023/distill_3</id><content type="html" xml:base="https://ginnyintifa.github.io/blog/2023/distill_3/"><![CDATA[<h2 id="why-pca">Why PCA?</h2> <p>Whenever I get a big data table, with features in rows and sample in columns, I would like to first see how the samples are clustered or distributed in the feature space in an unsupervised way. However, feature space can be huge, and not visualizable in this 3D world. PCA is a go-to choice to achieve a lower dimension (2 or 3) summerisation/representation of the original matrix. In a 2D space, the x axis could be the first principal component (PC) of the data and the y axis could be the second PC. I make a scatter plot, projecting the data points (samples) to these two axis.</p> <p>And I would color the data points regarding to some sample meta information, for example, tissue type, experiment batch, gender, marker mutation status. This type of visualization is usually very informative in many ways. It tells me if the features capture the intrinsic differences/characteristics of the samples, or in QC, I can directly spot batch effect if there is any. Even better, I would investigate the loadings of the PCs to see which features contribute how much and to what direction of these PCs.</p> \[maybe a PCA example ## PCA as a way to explain variance in the data When a data analyst is asked to interpret a PCA plot, for example the above one. He or she would probably say, samples are clustered according to their tissue type along the first PC and according to xx along the second PC, the first PC explains xx% of the total variance and the second explains xx%. In total we are examining xx% of the variance. We know that PCs are linear combinations of the original features and the first PC would explain more variance than the second one and the second would explain more than the third one. But how are these linear combinations determined and why they "explain" variance? As you may know, PCs are orthogonal to each other, and why is that? And how many PCs would there be for each data matrix? ## PCA and the sample covariance matrix Let's say the data matarix is $D$, with $p$ rows (features) and $n$ samples after you center the data such that features for each sample has mean at zero, you can get a centered matrix $X$. To calculate sample covariance matrix you can just easily calculate the matrix product ${1 \over n-1}XX^T$. Let's ignore the scalar for now. $XX^T$ would be a $p$ by $p$ symmetric matrix. Why is the covariance matrix important? Because it basically telling you the relationships between each pair of samples using ALL the features. Let's make $A = XX^T$. In linear algebra, a symmetric matrix has some very good properties. For one, it can be factorized as $A = Q\Lambda Q^T$. Where $Q$ is an orthogonal matrix, of which the columns are the eigenvectors of $A$. $\Lambda$ is a diagonal matrix with the eigenvalues of $A$ on its diagonal. And there are some very interesting properties of eigenvalues and eigenvectors. Basically they can capture the most fundamental characteristics of a matrix. And why is that? @@@ I need to better explain this part Ok, now it seems we have explored the calculation behind PCA. But what how is it actually processed in a modern computation program? Singular value decomposition! ## PCA and singular value decompostion It is very computationally expensive to calculate the covariance matrix, especially with dimensions are large. Singular value decomposition (SVD) comes in handy because there are fast and efficient ways to do that decomposition. What's better, SVD can be performed on any matrix (no need to be symmetric, even no need to be a square matrix). So for our centered data matrix $X$, we can SVD it to this form: $X = U\Sigma V^T$ Here, $U$ and $V$ are two orthogonal matrices, and $\Sigma$ is a diagonal matrix. This is exactly why math is elegant. For any matrix, you can view its impact as a rotation ($U$), followed by a strech ($\Sigma$), followed by another rotation ($V^T$). Then what about the our covariance matrix? $XX^T = (U\Sigma V^T)(U\Sigma V^T)^T = U\Sigma V^TV\Sigma^TU^T = U\Sigma^2U^T$ Note that the last equation is because $U$ is orthagonal. And guess what, we already know : $A = XX^T = Q\Lambda Q^T$ So now we have: $U\Sigma^2U^T = Q\Lambda Q^T$ Naturally we have: $U = Q$ and $\Sigma^2 = \Lambda$ This derivation tells us, if we want to get the eigenvalues and vectors of the covariance matrix, we just need to do a SVD on the centered data matrix. Isn't that wonderful? ## Glue together with some experiment in R You are probably famillar with the `prcomp` function in R. `svd` is the base function in R for SVD calculation. And let'd do a simple experiment to see the relationship between SVD and PCA analysis. Supose we have such a simple matrix $X$ $\begin{bmatrix}3 &amp; 4 &amp; 2\\4 &amp; 6 &amp; 1\\ 2 &amp; 5 &amp; 7\\ 6 &amp; 7&amp; 2\\2 &amp; 1&amp;4 \end{bmatrix}$ 3 samples and 5 features.\]]]></content><author><name></name></author><summary type="html"><![CDATA[Deeper than most data analysts.]]></summary></entry><entry><title type="html">Capture net PTM changes with proteomics data</title><link href="https://ginnyintifa.github.io/blog/2023/distill_2/" rel="alternate" type="text/html" title="Capture net PTM changes with proteomics data"/><published>2023-06-03T00:00:00+00:00</published><updated>2023-06-03T00:00:00+00:00</updated><id>https://ginnyintifa.github.io/blog/2023/distill_2</id><content type="html" xml:base="https://ginnyintifa.github.io/blog/2023/distill_2/"><![CDATA[<h2 id="why-post-translational-modification-ptm">Why post-translational modification (PTM)?</h2> <p>With the advance of PTM data acquisition technology, people are able to compare the PTM intensity between two groups of interest. One of the most seen PTM data is phosphorylation data. Signal transduction in key biological pathways rely on phosphorylation. For example, autophosphorylation of the C terminal of EGFR tyrosine sites are the start of epidermal growth factor signal transduction. In the scenario of Tumor vs Normal group comparison, it will be very informative to identify phosphorylation events that are significantly more intense in the Tumor group. Because, potentially and naively, if the abnormally high phosphorylation is causal, we could design kinase inhibitor to reduce the intensity of signal transduction.</p> <h2 id="typical-protocol-in-aquring-both-global-proteome-and-phosphoproteome">Typical protocol in aquring both global proteome and phosphoproteome</h2> <p>The following diagram a typical CPTAC phosphoryaltion data acquisition workflow. <img src="/assets/img/protocol.png" alt="protocol"/></p> <p>So you can see that phosphorylation is measured after TMT labelling. We make sure that the total amount of protein mass are equalized between channels, at least when at the pipetting stage, but there is no way to control for the same amount of total phosphorylation. On the other hand, for a certain phosphorylation intensithy comparison between two samples groups, we are not certain if the phosphorylation MS readout reflects real phospho difference or just the global protein difference.</p> <p>If we were to plot a scatter plot with protein abundance on the x axis and phosphorylation intensity on the y axis, across all the phosphorylation sites and all samples in a cohort, you would get a plot like this.</p> <p><img src="/assets/img/prot_phospho.png" alt="phospho"/></p> <h2 id="normalization-by-regression">Normalization by regression</h2> <p>If we want to compare the net phosphorylation change, we can simply fit this relationship with a linear regression and the residual will be the normalized phosphorylation intensity. After fitting the regression, the residual is supposed to be randomly distributed against protein abundance, which is what we want to achieve. Additionally, this regression can be fitted for all the sites, for all the sites on the same protein, or for each site. I recommend doing it for all the sites across all the proteins for robustness.</p> <p><img src="/assets/img/prot_phospho_after.png" alt="phospho"/></p> <h2 id="normalization-by-subtraction">Normalization by subtraction</h2> <p>There is a simpler way to do this correction without fitting a regression. In the MSstatsPTM paper(Kohler, Devon, et al. “MSstatsPTM: statistical relative quantification of posttranslational modifications in bottom-up mass spectrometry-based proteomics.” Molecular &amp; Cellular Proteomics 22.1 (2023)), mean protein abundance is substracted from the mean phospho intensity in both groups for each PTM site. Equivalently, this is the ratio between ratios (modifed and unmodified).</p> <p><img src="/assets/img/msstats.png" alt="msstats"/></p> <h2 id="what-is-the-major-difference-between-the-two-approaches">What is the major difference between the two approaches?</h2> <p>These two approaches are similar, if the coefficient from the regression is forced to be 1, then it will be basically the second method.</p> <p>Suppose a scenario where at the first place, PTM intensity and protein abundance are not correlated at all. With the first approach, adjustment would be neglegible. But with the second approach, the adjustment would make much bigger difference, after the adjustment, PTM intensity maybe negatively correlated with protein abundance.</p> <p>On the other hand, the second approach may not give enough adjustment if the coefficient is way larger than 1, which is not very likely though.</p> <h2 id="more-readings">More readings</h2> <p>Why normalize: Wu, Ronghu, et al. “Correct interpretation of comprehensive phosphorylation dynamics requires normalization by protein expression changes.” Molecular &amp; cellular proteomics 10.8 (2011)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[What many research groups are still missing.]]></summary></entry><entry><title type="html">Compare ratios of different proteins in the same sample</title><link href="https://ginnyintifa.github.io/blog/2023/distill/" rel="alternate" type="text/html" title="Compare ratios of different proteins in the same sample"/><published>2023-05-22T00:00:00+00:00</published><updated>2023-05-22T00:00:00+00:00</updated><id>https://ginnyintifa.github.io/blog/2023/distill</id><content type="html" xml:base="https://ginnyintifa.github.io/blog/2023/distill/"><![CDATA[<h2 id="tmt-background">TMT background</h2> <p>TMT is short for tandem mass tag. When you deal with proteomics data with large sample size (N &gt;= 20), you are most likely dealing with TMT data. TMT is a very smart way to achieve efficient quantification of proteins. In a TMT10-plex experiment, 10 channels (or samples) are being quantified simultaneously, so that technical artefact is minimized (compared to doing 10 times separately).</p> <p>Usually, you would have multiple plexes. Say you have 50 samples of interest, you may have more than 5 TMT-10 plexes in the experiment. How to control for the technial artefact between the 5 plexes? You may assign the last channel of each plex (or even more than 1 channel) to be a physical average of all the samples of interest. By doing so, you have a common reference in each of the plex, which to be leveraged in merging plexes together.</p> <p><a href="https://fragpipe.nesvilab.org/">FragPipe</a> , the best proteomics analysis platform, calculates a ratio between each channel to the reference channel, and these ratios then directly merged (think a <code class="language-plaintext highlighter-rouge">join</code>) across different plexes to generate a single data table, with each row being a gene/protein, and each column a sample.</p> <h2 id="compare-ratios-horizontally-between-samples-within-a-protein">Compare ratios horizontally between samples within a protein.</h2> <p>Now we are happy to see that within each gene (row), ratios can be safely compared between samples. You could conduct a differential expression analysis (DE) between groups of interest.</p> <h2 id="compare-ratios-vertically-between-proteins-within-a-sample">Compare ratios vertically between proteins within a sample.</h2> <p>But can I compare different genes within a single sample (column)? Would such analysis make any sense? I have to say, during my 3+ years experience of analysing TMT data, I never saw anyone directly performed such comparison, nor did anyone asked for such analysis. It just seems to be very weird!!! But let me tell you, <em>it makes perfect sense!</em></p> <p>Why? We subtracted the intensity of the bridge channel (physical average) from each channel, then the ratio is a relative measurement telling you how big or small this gene/protein is expressed for this sample, compared to an average level.</p> <p>In fact, the bridge sample serves as a mediator in eliminating many types of biases such as ionization efficiency and also accounts for the stochastic nature of MS/MS.</p> <p>Actually if we want to perform a ssGSEA (<a href="https://www.nature.com/articles/nature08460">original method</a>), we are going to use this ratio data.</p> <h2 id="a-real-example">A real example</h2> <p>I grabbed proteomics from a CPTAC lung squamous cell carcinoma (<a href="https://www.cell.com/cell/fulltext/S0092-8674(21)00857-6">LSCC)</a> project . In this dataset, we have both tumor samples and adjacent normal tissues (NAT). Supposedly, normal samples are more homogeneous to each other comparing to tumor samples (intertumoral heterogeneity). In the LSCC dataset, each plex was assigned with balanced tumor and normal sample channels. The bridge channels, i.e. a common reference, physical average of all samles, was used to adjust for between-plex difference (batch effect). We would expect the proteins expressed highly in one NAT sample relative to other proteins are also highly expressed in other NAT samples. To test it, I replace the actual ratios of protein for each sample with the ranks from high to low, I plot all the protein ranks against the rank of a randomly picked sample.</p> <p>It looks like this:</p> <p><img src="/assets/img/rank_1vsall.png" alt="rank"/></p> <p>The interpretation would be: if we randomly rank the protein ratios of one sample from high to low, then the sum of ranks over all the samples roughly goes from high to low as well, indicating it is justifiable to compare ratios of different proteins in the same sample.</p> <p>I did a ssGSEA with the ratios and here is a heatmap visualizating the results:</p> <p><img src="/assets/img/ssGSEA.png" alt="ssGSEA"/></p> <p>Ratios are “rankable” values across proteins. Notice that the left most patient was excluded in downstream analysis due to sample quality.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[not what we normally do with TMT data]]></summary></entry><entry><title type="html">A pair of analytical eyes</title><link href="https://ginnyintifa.github.io/blog/2023/formatting-and-links/" rel="alternate" type="text/html" title="A pair of analytical eyes"/><published>2023-02-18T00:00:00+00:00</published><updated>2023-02-18T00:00:00+00:00</updated><id>https://ginnyintifa.github.io/blog/2023/formatting-and-links</id><content type="html" xml:base="https://ginnyintifa.github.io/blog/2023/formatting-and-links/"><![CDATA[<p>After earning my PhD in bioinformatics/biostatistics from the National University of Singapore, I was happy with the successful completion of my <a href="https://scholar.google.com/citations?user=q271nRcAAAAJ&amp;hl=en">two major projects</a>. However, as a math graduate (my undergraduate major), I couldn’t shake the feeling that those projects may not have been as “mathematical” as I had initially anticipated. Much of my time was spent cleaning up raw omics datasets, including maf files, RNAseq counts data, LFQ and TMT proteomics quantifications.</p> <p>My advisor, a brilliant statistical mind, reassured me that my skills were not to be compared with those of CS or math PhDs. Rather, my analytical eyes, honed over five years of working with a plethora of omics datasets, would allow me to make significant discoveries that even “smart” CS and math people might struggle to approach.</p> <p>Encouraged by my advisor’s words, I applied for and landed a postdoctoral position at the University of Michigan. To my great fortune and gratitude, I was given the opportunity to lead a large tumor cohort study as the primary bioinformatician. Immediately, I was tasked with managing genomics, transcriptomics, proteomics, and metabolomics data from over 200 patient samples, which I had to filter, normalize, impute, and interpret.</p> <p>Working closely with over 20 collaborators, I discovered that I had a natural talent for the job at hand. My analytical eyes proved to be an asset in navigating through the extensive and, at times, messy datasets. Through this experience, I found a newfound appreciation for my own abilities and grew to love myself more.</p> <p>I always hoped to document what I have learned throughout the years, but was unsure of how to do so. As fate would have it, I met <a href="https://crazyhottommy.github.io/">Tommy</a>, a generous computational biologist who loves to share knowledge. He encouraged me to start my own blog, and so here am I, sharing my humble experience in bioinformatics with you.</p>]]></content><author><name></name></author><category term="about-me"/><category term="first_blog"/><summary type="html"><![CDATA[my first blog]]></summary></entry></feed>